{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n",
      "\n",
      "\n",
      "\n",
      ">>> Starting Redshift Cluster!\n",
      ">>> This is the configuration of redshift cluster!\n",
      "\n",
      "\n",
      "                    Param       Value\n",
      "0  DWH_CLUSTER_TYPE        multi-node\n",
      "1  DWH_NUM_NODES           4         \n",
      "2  DWH_NODE_TYPE           dc2.large \n",
      "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
      "4  DWH_DB                  dwh       \n",
      "5  DWH_DB_USER             dwhuser   \n",
      "6  DWH_DB_PASSWORD         Passw0rd  \n",
      "7  DWH_PORT                5439      \n",
      "8  DWH_IAM_ROLE_NAME       dwhRole   \n",
      "\n",
      "\n",
      "\n",
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "\n",
      "\n",
      ">>> arn:aws:iam::539761204517:role/dwhRole\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8099/1335240974.py:135: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ">>> Cluster successfully created!\n",
      "\n",
      "\n",
      ">>> DWH_ENDPOINT :: \n",
      " dwhcluster.cmjnfq1cdb24.us-west-2.redshift.amazonaws.com\n",
      "\n",
      ">>> DWH_ROLE_ARN :: \n",
      " arn:aws:iam::539761204517:role/dwhRole\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "postgresql://dwhuser:Passw0rd@dwhcluster.cmjnfq1cdb24.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3.ObjectSummary(bucket_name='tien-duong1151', key='data/')!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3.ObjectSummary(bucket_name='tien-duong1151', key='data/binance_btc.csv')!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3.ObjectSummary(bucket_name='tien-duong1151', key='data/coin_exchange_info.csv')!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3.ObjectSummary(bucket_name='tien-duong1151', key='data/coin_market_info.csv')!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3.ObjectSummary(bucket_name='tien-duong1151', key='data/coins_data.csv')!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3.ObjectSummary(bucket_name='tien-duong1151', key='data/demo_file.csv')!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import psycopg2\n",
    "import time\n",
    "%load_ext sql\n",
    "\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "KEY                    = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "SECRET                 = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = os.getenv(\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = os.getenv(\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = os.getenv(\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = os.getenv(\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = os.getenv(\"DWH_DB\")\n",
    "DWH_DB_USER            = os.getenv(\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = os.getenv(\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = os.getenv(\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = os.getenv(\"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "print(\"\\n\\n\")\n",
    "print(\">>> Starting Redshift Cluster!\\n>>> This is the configuration of redshift cluster!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             }))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(f\"\\n\\n>>> {roleArn}\\n\\n\")\n",
    "\n",
    "# try:\n",
    "#     response = redshift.create_cluster(        \n",
    "#         #HW\n",
    "#         ClusterType=DWH_CLUSTER_TYPE,\n",
    "#         NodeType=DWH_NODE_TYPE,\n",
    "#         NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "#         #Identifiers & Credentials\n",
    "#         DBName=DWH_DB,\n",
    "#         ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "#         MasterUsername=DWH_DB_USER,\n",
    "#         MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "#         #Roles (for s3 access)\n",
    "#         IamRoles=[roleArn]  )\n",
    "# except Exception as e:\n",
    "#     print(f\"\\n >>> {e} \\n\\n\")\n",
    "    \n",
    "    \n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "\n",
    "cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "\n",
    "check_status = cluster_df['Value'][2]\n",
    "status = 'available'\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "    cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "    check_status = cluster_df['Value'][2]\n",
    "    if check_status == status:\n",
    "        DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "        DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "        print(\"\\n\\n>>> Cluster successfully created!\\n\")\n",
    "        print(\"\\n>>> DWH_ENDPOINT :: \\n\", DWH_ENDPOINT)\n",
    "        print(\"\\n>>> DWH_ROLE_ARN :: \\n\", DWH_ROLE_ARN)\n",
    "        break\n",
    "    print('Cluster not up yet')\n",
    "\n",
    "\n",
    "\n",
    "conn_string=\"\\n\\npostgresql://{}:{}@{}:{}/{}\\n\\n\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(f\"\\n\\n{conn_string}\\n\\n\")\n",
    "%sql $conn_string\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "\n",
    "sampleDbBucket =  s3.Bucket(\"tien-duong1151\")\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"data/\"):\n",
    "     print(f\"\\n\\n{obj}!\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_stage_table_drop = 'DROP TABLE IF EXISTS staging_demo'\n",
    "demo_stage_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_demo (\n",
    "name VARCHAR\n",
    ",base VARCHAR \n",
    ",quote VARCHAR\n",
    ",price VARCHAR);\n",
    "\"\"\")\n",
    "my_query = [demo_stage_table_drop, demo_stage_table_create]\n",
    "\n",
    "conn = psycopg2.connect(f\"host={DWH_ENDPOINT} dbname={DWH_DB} user={DWH_DB_USER} password={DWH_DB_PASSWORD} port={DWH_PORT}\")\n",
    "cur = conn.cursor()\n",
    "for i in my_query:\n",
    "    cur.execute(i)\n",
    "    conn.commit()\n",
    "conn.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cmjnfq1cdb24.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "(psycopg2.errors.SyntaxError) syntax error at or near \"'auto'\"\n",
      "LINE 1: ...Role' COMPUPDATE OFF region 'us-west-2' FORMAT AS csv 'auto'\n",
      "                                                                 ^\n",
      "\n",
      "[SQL: COPY staging_demo FROM 's3://tien-duong1151/data/demo_file.csv' CREDENTIALS 'aws_iam_role=arn:aws:iam::539761204517:role/dwhRole' COMPUPDATE OFF region 'us-west-2' FORMAT AS csv 'auto']\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "my_bucket = 's3://tien-duong1151/data/demo_file.csv'\n",
    "\n",
    "staging_demo = f\"\"\"\n",
    "COPY staging_demo \n",
    "FROM '{my_bucket}'\n",
    "CREDENTIALS 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "COMPUPDATE OFF region 'us-west-2'\n",
    "FORMAT AS csv 'auto'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "%sql $staging_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cmjnfq1cdb24.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bucket = 's3://tien-duong1151/data/demo_file.csv'\n",
    "DWH_ROLE_ARN = \"arn:aws:iam::539761204517:role/dwhRole\"\n",
    "staging_demo = \"\"\"\n",
    "COPY staging_demo FROM 's3://tien-duong1151/data/demo_file.csv' \n",
    "CREDENTIALS 'aws_iam_role=arn:aws:iam::539761204517:role/dwhRole'\n",
    "IGNOREHEADER 1\n",
    "delimiter ',' \n",
    "blanksasnull\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "%sql $staging_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 290 entries, 0 to 289\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              290 non-null    int64  \n",
      " 1   name            290 non-null    object \n",
      " 2   name_id         290 non-null    object \n",
      " 3   url             290 non-null    object \n",
      " 4   country         290 non-null    object \n",
      " 5   date_live       220 non-null    object \n",
      " 6   date_added      290 non-null    object \n",
      " 7   usdt            290 non-null    int64  \n",
      " 8   fiat            290 non-null    int64  \n",
      " 9   auto            290 non-null    object \n",
      " 10  volume_usd      290 non-null    float64\n",
      " 11  udate           251 non-null    object \n",
      " 12  volume_usd_adj  290 non-null    float64\n",
      "dtypes: float64(2), int64(3), object(8)\n",
      "memory usage: 29.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv('data/coin_exchange_info.csv')\n",
    "dft.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7000 entries, 0 to 6999\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  7000 non-null   int64  \n",
      " 1   symbol              7000 non-null   object \n",
      " 2   name                7000 non-null   object \n",
      " 3   nameid              7000 non-null   object \n",
      " 4   rank                7000 non-null   float64\n",
      " 5   price_usd           7000 non-null   float64\n",
      " 6   percent_change_24h  7000 non-null   float64\n",
      " 7   percent_change_1h   6984 non-null   float64\n",
      " 8   percent_change_7d   6994 non-null   float64\n",
      " 9   price_btc           7000 non-null   float64\n",
      " 10  market_cap_usd      7000 non-null   object \n",
      " 11  volume24            7000 non-null   float64\n",
      " 12  volume24a           4496 non-null   float64\n",
      " 13  csupply             7000 non-null   object \n",
      " 14  tsupply             5867 non-null   float64\n",
      " 15  msupply             4129 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(5)\n",
      "memory usage: 875.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv('data/coin_data.csv')\n",
    "dft.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 992 entries, 0 to 991\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   name        974 non-null    object \n",
      " 1   base        974 non-null    object \n",
      " 2   quote       974 non-null    object \n",
      " 3   price       974 non-null    float64\n",
      " 4   price_usd   974 non-null    float64\n",
      " 5   volume      974 non-null    float64\n",
      " 6   volume_usd  974 non-null    float64\n",
      " 7   time        974 non-null    object \n",
      "dtypes: float64(4), object(4)\n",
      "memory usage: 62.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv('data/coin_market_info.csv')\n",
    "dft.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.to_csv('data/.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_credentials = '123456'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f'my_credential = {my_credentials}'\n",
    "b = 'moreoutput'\n",
    "c = 'something more then this'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open('.env_db', 'w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        # print(f'DWH_ENDPOINT={DWH_ENDPOINT}')\n",
    "        # print(f'DWH_ROLE_ARN={DWH_ROLE_ARN}')\n",
    "        print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "client = boto3.client('s3')\n",
    "resource = boto3.resource('s3')\n",
    "bucket='tien-duong1151'\n",
    "s3_file = 'data/new_exchange_file.csv'\n",
    "\n",
    "key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "secret = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "obj = client.get_object(Bucket=bucket, Key=s3_file)\n",
    "my_obj = pd.read_csv(obj['Body'])\n",
    "my_obj.to_csv('data/coin_exchange_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 992 entries, 0 to 991\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  992 non-null    int64  \n",
      " 1   name        974 non-null    object \n",
      " 2   base        974 non-null    object \n",
      " 3   quote       974 non-null    object \n",
      " 4   price       974 non-null    float64\n",
      " 5   price_usd   974 non-null    float64\n",
      " 6   volume      974 non-null    float64\n",
      " 7   volume_usd  974 non-null    float64\n",
      " 8   time        974 non-null    object \n",
      "dtypes: float64(4), int64(1), object(4)\n",
      "memory usage: 69.9+ KB\n"
     ]
    }
   ],
   "source": [
    "my_obj.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def upload_to_s3(bucketname, local_file_path, s3_file_path):\n",
    "    \n",
    "    try:\n",
    "        s3 = boto3.resource('s3')\n",
    "        s3.meta.client.upload_file(local_file_path, bucketname, s3_file_path)\n",
    "        print('Success!')\n",
    "    except Exception as e:\n",
    "        print(f'{e}\\n Fail!')\n",
    "s3_file = 'data/coin_market_info.csv'\n",
    "my_local = '/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/data/coin_market_info.csv'\n",
    "\n",
    "upload_to_s3(bucket, my_local, s3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ffc6fd8cb3e14e16b5dc077588a22483e644787fd42fd05cc460417ce2d8385"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('capstone_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
