                                # coinlore_api_project
## README Project Overview

As part of Data-Engineer Udacity-CapStone project to apply concept learn within the course such as AWS services, relational database systems, and scheduling services. With the current popularity crypto-currencie. To help user keeping up to date of new coins being created and different exchanges services. I created an data pipeline from coinlore api service and hosted it on AWS services including dataware house storage system.Coinlore is an open public API that let user request for cryptocurrencies and exchanges information. This project current lack many feature in supporting real-time situation data. Intergrating features such as data migration tools to keeping track of different versions and migration control. Furthermore, the project can also leverages of scheduling services to refresh the database with the new data inserted. 

## Pipeline Steps
- Build ETL pipeline from coinlore open API of cryptopcurrencies
- Transform requested data into CSV format and manipulate the data with pyspark, pandas, and SQL
- Stores the requested data into S3 bucket as temperorary file's storage
- Create a pipeline to transfer CSV file format into RedShift serves as dataware
- Stages data into Redshift AWS as staging table
- Use staging tables to create database as relational Star Schema concept

#### Tool used
- Python
- Power bi
- Jupyter Notebook
- Postgres sql
- AWS S3
- AWS Redshift

## Data Information
- 

## DataSet info
Song data: s3://udacity-dend/song_data
 - Each file is in JSON format and contains metadata about a song and the artist of that song.
 
Log data: s3://udacity-dend/log_data
-  log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.


### Current Project Files
 
 1. ELT.py  --> Responsible for execution of extract, tranform and load the dataset from S3 back into S3 processes using Spark
 
 2. dl.cfg --> Contains AWS credentials
 