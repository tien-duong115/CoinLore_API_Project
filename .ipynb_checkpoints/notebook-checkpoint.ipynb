{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powerbiclient import Report, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n",
      " To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FLJF4WW4W to authenticate.\n",
      "You have logged in.\n",
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "from powerbiclient.authentication import DeviceCodeLoginAuthentication\n",
    "device_auth = DeviceCodeLoginAuthentication()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id=\"me/list\"\n",
    "report_id=\"0d8cc3a7-23f2-443e-b3da-ea26a9994452\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "('Could not create access token or embed URL: ', Exception('Get embed URL failed with status code ', 404))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\powerbiclient\\report.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, access_token, embed_url, token_type, group_id, report_id, auth, embed_token_request_body, view_mode, permissions, client_id, tenant, scopes, dataset_id, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Get embed URL failed with status code \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m                     \u001b[0membed_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embedUrl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: ('Get embed URL failed with status code ', 404)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-54ffd84c8994>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice_auth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\powerbiclient\\report.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, access_token, embed_url, token_type, group_id, report_id, auth, embed_token_request_body, view_mode, permissions, client_id, tenant, scopes, dataset_id, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m                     \u001b[0membed_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embedUrl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not create access token or embed URL: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Tells if Power BI events are being observed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: ('Could not create access token or embed URL: ', Exception('Get embed URL failed with status code ', 404))"
     ]
    }
   ],
   "source": [
    "report = Report(group_id=group_id, report_id=report_id, auth=device_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      ">>> Starting Redshift Cluster!\n",
      ">>> This is the configuration of redshift cluster!\n",
      "\n",
      "\n",
      "                    Param       Value\n",
      "0        DWH_CLUSTER_TYPE  multi-node\n",
      "1           DWH_NUM_NODES           4\n",
      "2           DWH_NODE_TYPE   dc2.large\n",
      "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
      "4                  DWH_DB         dwh\n",
      "5             DWH_DB_USER     dwhuser\n",
      "6         DWH_DB_PASSWORD    Passw0rd\n",
      "7                DWH_PORT        5439\n",
      "8       DWH_IAM_ROLE_NAME     dwhRole\n",
      "\n",
      "\n",
      "\n",
      "1.1 Creating a new IAM Role\n",
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "\n",
      "\n",
      ">>> arn:aws:iam::539761204517:role/dwhRole\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/pandas_functions.py:103: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "\n",
      "\n",
      ">>> Cluster successfully created!\n",
      "\n",
      "\n",
      ">>> DWH_ENDPOINT :: dwhcluster.cmjnfq1cdb24.us-west-2.redshift.amazonaws.com\n",
      "\n",
      "\n",
      ">>> DWH_ROLE_ARN :: arn:aws:iam::539761204517:role/dwhRole\n",
      "\n",
      "ec2.SecurityGroup(id='sg-fa39a2f3')\n",
      "\n",
      ">>> An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/capstone_venv/bin/python3.8\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pandas_functions import prettyRedshiftProps\n",
    "import config as c\n",
    "from contextlib import redirect_stdout\n",
    "import os\n",
    "\n",
    "exe_path = '/mnt/c/Users/tienl/Udacity_Courses/DE_capstone'\n",
    "os.chdir(exe_path)\n",
    "\n",
    "# (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "print(\"\\n\\n\")\n",
    "print(\">>> Starting Redshift Cluster!\\n>>> This is the configuration of redshift cluster!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [c.DWH_CLUSTER_TYPE, c.DWH_NUM_NODES, c.DWH_NODE_TYPE, c.DWH_CLUSTER_IDENTIFIER, c.DWH_DB, c.DWH_DB_USER, c.DWH_DB_PASSWORD, c.DWH_PORT, c.DWH_IAM_ROLE_NAME]\n",
    "             }))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "ec2 = c.ec2\n",
    "s3 = c.s3\n",
    "iam = c.iam\n",
    "redshift = c.redshift\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=c.DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=c.DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=c.DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=c.DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(f\"\\n\\n>>> {roleArn}\\n\\n\")\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=c.DWH_CLUSTER_TYPE,\n",
    "        NodeType=c.DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(c.DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=c.DWH_DB,\n",
    "        ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=c.DWH_DB_USER,\n",
    "        MasterUserPassword=c.DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  )\n",
    "except Exception as e:\n",
    "    print(f\"\\n >>> {e} \\n\\n\")\n",
    "    \n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "\n",
    "cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "\n",
    "check_status = cluster_df['Value'][2]\n",
    "status = 'available'\n",
    "\n",
    "\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "    cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "    check_status = cluster_df['Value'][2]\n",
    "    if check_status == status:\n",
    "        DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "        DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "        print(\"\\n\\n>>> Cluster successfully created!\\n\")\n",
    "        print(f\"\\n>>> DWH_ENDPOINT :: {DWH_ENDPOINT}\\n\")\n",
    "        print(f\"\\n>>> DWH_ROLE_ARN :: { DWH_ROLE_ARN}\\n\")\n",
    "        break\n",
    "    print('Cluster not up yet')\n",
    "\n",
    "\n",
    "with open('database/.env', 'w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        print(f'DWH_ENDPOINT={DWH_ENDPOINT}')\n",
    "        print(f'DWH_ROLE_ARN={DWH_ROLE_ARN}')\n",
    "     \n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(c.DWH_PORT),\n",
    "        ToPort=int(c.DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n>>> {e}\\n\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request data from CoinLore API and load into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported to data/coin_exchange_info.csv!\n",
      "Sucessfully get data and export out to {coin_exchange_path}!\n",
      "Successfully request number 0\n",
      "Successfully request number 100\n",
      "Successfully request number 200\n",
      "Successfully request number 300\n",
      "Successfully request number 400\n",
      "Successfully request number 500\n",
      "Successfully request number 600\n",
      "Successfully request number 700\n",
      "Successfully request number 800\n",
      "Successfully request number 900\n",
      "Successfully request number 1000\n",
      "Successfully request number 1100\n",
      "Successfully request number 1200\n",
      "Successfully request number 1300\n",
      "Successfully request number 1400\n",
      "Successfully request number 1500\n",
      "Successfully request number 1600\n",
      "Successfully request number 1700\n",
      "Successfully request number 1800\n",
      "Successfully request number 1900\n",
      "Successfully request number 2000\n",
      "Successfully request number 2100\n",
      "Successfully request number 2200\n",
      "Successfully request number 2300\n",
      "Successfully request number 2400\n",
      "Successfully request number 2500\n",
      "Successfully request number 2600\n",
      "Successfully request number 2700\n",
      "Successfully request number 2800\n",
      "Successfully request number 2900\n",
      "Successfully request number 3000\n",
      "Successfully request number 3100\n",
      "Successfully request number 3200\n",
      "Successfully request number 3300\n",
      "Successfully request number 3400\n",
      "Successfully request number 3500\n",
      "Successfully request number 3600\n",
      "Successfully request number 3700\n",
      "Successfully request number 3800\n",
      "Successfully request number 3900\n",
      "Successfully request number 4000\n",
      "Successfully request number 4100\n",
      "Successfully request number 4200\n",
      "Successfully request number 4300\n",
      "Successfully request number 4400\n",
      "Successfully request number 4500\n",
      "Successfully request number 4600\n",
      "Successfully request number 4700\n",
      "Successfully request number 4800\n",
      "Successfully request number 4900\n",
      "Successfully request number 5000\n",
      "Successfully request number 5100\n",
      "Successfully request number 5200\n",
      "Successfully request number 5300\n",
      "Successfully request number 5400\n",
      "Successfully request number 5500\n",
      "Successfully request number 5600\n",
      "Successfully request number 5700\n",
      "Successfully request number 5800\n",
      "Successfully request number 5900\n",
      "Successfully request number 6000\n",
      "Successfully request number 6100\n",
      "Successfully request number 6200\n",
      "Successfully request number 6300\n",
      "Successfully request number 6400\n",
      "Successfully request number 6500\n",
      "Successfully request number 6600\n",
      "Successfully request number 6700\n",
      "Successfully request number 6800\n",
      "Successfully request number 6900\n",
      "exported to data/coins_data.csv!\n",
      "Sucessfully get data and export out to {coin_path}!\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=90\n",
      "\n",
      "Request number: 0\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=80\n",
      "\n",
      "Request number: 1\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2710\n",
      "\n",
      "Request number: 2\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=518\n",
      "\n",
      "Request number: 3\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48543\n",
      "\n",
      "Request number: 4\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=257\n",
      "\n",
      "Request number: 5\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33285\n",
      "\n",
      "Request number: 6\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=58\n",
      "\n",
      "Request number: 7\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48537\n",
      "\n",
      "Request number: 8\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=45219\n",
      "\n",
      "Request number: 9\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=44883\n",
      "\n",
      "Request number: 10\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2\n",
      "\n",
      "Request number: 11\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=45088\n",
      "\n",
      "Request number: 12\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48591\n",
      "\n",
      "Request number: 13\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33422\n",
      "\n",
      "Request number: 14\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=47305\n",
      "\n",
      "Request number: 15\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=34406\n",
      "\n",
      "Request number: 16\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=1\n",
      "\n",
      "Request number: 17\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2751\n",
      "\n",
      "Request number: 18\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2321\n",
      "\n",
      "Request number: 19\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48563\n",
      "\n",
      "Request number: 20\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33830\n",
      "\n",
      "Request number: 21\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48571\n",
      "\n",
      "Request number: 22\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=89\n",
      "\n",
      "Request number: 23\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33644\n",
      "\n",
      "Request number: 24\n",
      "\n",
      "exported to data/top_coins.csv!\n",
      "\n",
      "Sucessfully get data and export out to: {top_coins_path}!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/02 14:53:26 WARN Utils: Your hostname, DESKTOP-7O0VB5H resolves to a loopback address: 127.0.1.1; using 172.22.5.159 instead (on interface eth0)\n",
      "22/01/02 14:53:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/capstone_venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4f7628fa-cade-448c-9888-aac497c584d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 289ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4f7628fa-cade-448c-9888-aac497c584d5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/9ms)\n",
      "22/01/02 14:53:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/02 14:53:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: unix, date, symbol, open, high, low, close, Volume BTC, Volume USDT, tradecount\n",
      " Schema: unix, date, symbol, open, high, low, close, Volume BNB, Volume USDT, tradecount\n",
      "Expected: Volume BNB but found: Volume BTC\n",
      "CSV file: file:///mnt/c/Users/tienl/Udacity_Courses/DE_capstone/data/Downloaded_data/BTCUSDT_Binance_futures_data_minute.csv\n",
      "22/01/02 14:53:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: unix, date, symbol, open, high, low, close, Volume LTC, Volume USDT, tradecount\n",
      " Schema: unix, date, symbol, open, high, low, close, Volume BNB, Volume USDT, tradecount\n",
      "Expected: Volume BNB but found: Volume LTC\n",
      "CSV file: file:///mnt/c/Users/tienl/Udacity_Courses/DE_capstone/data/Downloaded_data/LTCUSDT_Binance_futures_data_minute.csv\n",
      "22/01/02 14:53:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: unix, date, symbol, open, high, low, close, Volume ETH, Volume USDT, tradecount\n",
      " Schema: unix, date, symbol, open, high, low, close, Volume BNB, Volume USDT, tradecount\n",
      "Expected: Volume BNB but found: Volume ETH\n",
      "CSV file: file:///mnt/c/Users/tienl/Udacity_Courses/DE_capstone/data/Downloaded_data/ETHUSDT_Binance_futures_data_minute.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sucessfully export file to: data/historical_data.csv!\n",
      "\n",
      "\n",
      "\n",
      "Sucessfully clean and export historical data to data/historical_data.csv\n",
      "\n",
      "Success!\n",
      "Successfully Uploaded data/final_historical_data.csvto S3!\n",
      "Success!\n",
      "\n",
      "Successfully Uploaded data/coins_data.csv to S3!\n",
      "\n",
      "Success!\n",
      "Successfully Uploaded data/top_coins.csvto S3!\n",
      "Success!\n",
      "Successfully Uploaded data/coin_exchange_info.csvto S3!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from requests.api import get\n",
    "from pandas_functions import *\n",
    "from  spark_functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import config as c\n",
    "import glob\n",
    "\n",
    "def main():\n",
    "    \"\"\"[\n",
    "        - Data pipelines to extract, tranform and load data into S3 bucket\n",
    "        ]\n",
    "    \"\"\"\n",
    "    \n",
    "    load_dotenv()\n",
    "    # file paths\n",
    "    coin_path = 'data/coins_data.csv'\n",
    "    top_coins_path = 'data/top_coins.csv'\n",
    "    coin_exchange_path = 'data/coin_exchange_info.csv'\n",
    "    historical_data = 'data/historical_data.csv'\n",
    "    historical_data_path = \"data/Downloaded_data/*.csv\"\n",
    "    final_historical = \"data/final_historical_data.csv\"\n",
    "    # Exchange data pipeline\n",
    "    try:\n",
    "        GET_all_exchanges = 'https://api.coinlore.net/api/exchanges/'\n",
    "        exchange_requests = get_request(GET_all_exchanges)\n",
    "        exchange_data = exchange_data_filter(exchange_requests)\n",
    "        export_csv(exchange_data,coin_exchange_path)\n",
    "        print(\"Sucessfully get data and export out to {coin_exchange_path}!\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to GET at {GET_all_exchanges}\\n')\n",
    "        print(e)\n",
    "        \n",
    "    # coins data pipeline \n",
    "    try:\n",
    "        coin_requests= get_coin_request(start=0, limit=7000)\n",
    "        export_csv(coin_requests, coin_path)\n",
    "        print(\"Sucessfully get data and export out to {coin_path}!\")\n",
    "    except Exception as e:\n",
    "        print('\\nFail to GET at {coin_path}\\n')\n",
    "        print(e)\n",
    "\n",
    "    #top coins data pipeline\n",
    "    try:\n",
    "        top_coins_path = 'data/top_coins.csv'\n",
    "        coin_market_data = top_rank_coins(HowMany=25)\n",
    "        export_csv(coin_market_data, top_coins_path)\n",
    "        print(\"\\nSucessfully get data and export out to: {top_coins_path}!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print('\\nFail to GET at {top_coins_path}\\n')\n",
    "        print(e)\n",
    "\n",
    "    # binance historical coins data clean and export to csv\n",
    "    try:\n",
    "        path = r'data/historical_data.csv'\n",
    "        binance_coins_data(historical_data_path, historical_data)\n",
    "        all_files = glob.glob(path + \"/*.csv\")\n",
    "        li = []\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            li.append(df)\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv('data/final_historical_data.csv', index=False)\n",
    "        print(f\"\\nSucessfully clean and export historical data to {historical_data}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to clean and export {historical_data_path}\\n')\n",
    "        print(e) \n",
    "    \n",
    "    # Stage historical 1 minutes data to s3\n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=final_historical, s3_file_path=final_historical)\n",
    "        print(f\"Successfully Uploaded {final_historical}to S3!\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {final_historical}\\n')\n",
    "        print(e)\n",
    "    \n",
    "    # stage coins_data info to s3\n",
    "    \n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=coin_path, s3_file_path=coin_path)\n",
    "        print(f\"\\nSuccessfully Uploaded {coin_path} to S3!\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {coin_path}\\n')\n",
    "        print(e)\n",
    "    \n",
    "    # stage coin market data into s3\n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=top_coins_path, s3_file_path=top_coins_path)\n",
    "        print(f\"Successfully Uploaded {top_coins_path}to S3!\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {top_coins_path}\\n')\n",
    "        print(e)\n",
    "    \n",
    "    # stage coin exchange path into s3\n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=coin_exchange_path, s3_file_path=coin_exchange_path)\n",
    "        print(f\"Successfully Uploaded {coin_exchange_path}to S3!\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {coin_exchange_path}\\n')\n",
    "        print(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From S3 stage data into Redshift and dimensional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS coins_data_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS exchange_data_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS top_coins_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS historical_data_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS exchange_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS coins_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      " DROP TABLE IF EXISTS historical_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      " DROP TABLE IF EXISTS top_coins_data_table!\n",
      "\n",
      "3: \n",
      "Invalid role ARN: None\n",
      "DETAIL:  \n",
      "  -----------------------------------------------\n",
      "  error:  Invalid role ARN: None\n",
      "  code:      30000\n",
      "  context:   \n",
      "  query:     121\n",
      "  location:  xen_aws_credentials_mgr.cpp:402\n",
      "  process:   padbmaster [pid=14025]\n",
      "  -----------------------------------------------\n",
      "\n",
      "\n",
      "TRYING INSERT AT: \n",
      " INSERT INTO top_coins_data_table( id, name, base, quote, price, price_usd, volume, volume_usd, time)\n",
      "SELECT  id::integer\n",
      "       ,name\n",
      "       ,base\n",
      "       ,quote\n",
      "       ,price::float\n",
      "       ,price_usd::float\n",
      "       ,volume::float\n",
      "       ,volume_usd::float\n",
      "       ,time::date\n",
      "FROM top_coins_table_stage\n",
      "\n",
      "4: \n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/capstone_venv/bin/python3.8\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from database.create_table_sql import drop_table_queries, create_table_queries, copy_table_queries, insert_table_queries\n",
    "\n",
    "exe_path = '/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/database'\n",
    "os.chdir(exe_path)\n",
    "#Dynamic Environment variables\n",
    "load_dotenv()\n",
    "HOST = os.getenv('DWH_ENDPOINT')\n",
    "DWH_ROLE_ARB=os.getenv('DWH_ROLE_ARB')\n",
    "\n",
    "#Local and Static Environment Variables\n",
    "my_path = Path('/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/.env')\n",
    "load_dotenv(my_path)\n",
    "localhost=os.getenv('localhost')\n",
    "local_dbname=os.getenv('local_dbname')\n",
    "local_password=os.getenv('local_password')\n",
    "local_username=os.getenv('local_username')\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_PORT = os.getenv(\"DWH_PORT\")\n",
    "S3_BUCKET_NAME=os.getenv('S3_BUCKET_NAME')\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function drop all tables if already exists\n",
    "    \"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        print(f\"\\nSucessfully DROPPED \\n{query}!\\n\")\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function creates table if not exists\n",
    "    \"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function load the staging tables into our DW\n",
    "    \"\"\"\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def insert_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function insert data into the tables in the DW\n",
    "    \"\"\"\n",
    "    for query in insert_table_queries:\n",
    "        print(f'TRYING INSERT AT: \\n{query}')\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Using another files with all connection string to connect into DW cluster\n",
    "    \"\"\"\n",
    "\n",
    "    conn = psycopg2.connect(f\"host={HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "    # conn = psycopg2.connect(host=f'{localhost}',database=f'{local_dbname}',user=f'{local_username}',password=f'{local_password}')\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        drop_tables(cur, conn)\n",
    "    except Exception as e:\n",
    "        print(f'1: \\n{e}')\n",
    "    try:      \n",
    "        create_tables(cur, conn)\n",
    "    except Exception as e:\n",
    "        print(f'2: \\n{e}')\n",
    "    try:        \n",
    "        load_staging_tables(cur,conn)\n",
    "        print(\"\\n\\nSucessfully COPY new tables!\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'3: \\n{e}')\n",
    "    try:\n",
    "        insert_tables(cur,conn)\n",
    "        print(\"Sucessfully inserted!\")\n",
    "    except Exception as e:\n",
    "        print(f'4: \\n{e}')\n",
    "\n",
    "            \n",
    "    conn.close()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> No cluster found!\n",
      ">>> Cluster deleted!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas_functions import prettyRedshiftProps\n",
    "import config as c\n",
    "\n",
    "def main():\n",
    "    \n",
    "    status = 'deleting'\n",
    "    c.redshift.delete_cluster( ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "    c.iam.detach_role_policy(RoleName=c.DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    c.iam.delete_role(RoleName=c.DWH_IAM_ROLE_NAME)\n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        myClusterProps = c.redshift.describe_clusters(ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "        cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "        check_status = cluster_df['Value'][2]\n",
    "        # check_status = cluster_df['Value'][2]\n",
    "        try:\n",
    "            if check_status == status:\n",
    "                print(f'\\n\\n>>> Cluster status: {check_status}!\\n')\n",
    "        except Exception as e:\n",
    "            if e == e:\n",
    "                print('\\n\\n>>> Cluster deleted!\\n')\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        if e ==e:\n",
    "            print('\\n\\n>>> No cluster found!\\n>>> Cluster deleted!\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ffc6fd8cb3e14e16b5dc077588a22483e644787fd42fd05cc460417ce2d8385"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
