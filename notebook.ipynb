{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **main.py**\n",
    "\n",
    "Make Requests to CoinLore API, cleaned data, and exported into S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported to data/coin_exchange_info.csv!\n",
      "\n",
      "\n",
      "Sucessfully get data and export out to data/coin_exchange_info.csv!\n",
      "\n",
      "\n",
      "Successfully request number 0\n",
      "Successfully request number 100\n",
      "Successfully request number 200\n",
      "Successfully request number 300\n",
      "Successfully request number 400\n",
      "Successfully request number 500\n",
      "Successfully request number 600\n",
      "Successfully request number 700\n",
      "Successfully request number 800\n",
      "Successfully request number 900\n",
      "Successfully request number 1000\n",
      "Successfully request number 1100\n",
      "Successfully request number 1200\n",
      "Successfully request number 1300\n",
      "Successfully request number 1400\n",
      "Successfully request number 1500\n",
      "Successfully request number 1600\n",
      "Successfully request number 1700\n",
      "Successfully request number 1800\n",
      "Successfully request number 1900\n",
      "Successfully request number 2000\n",
      "Successfully request number 2100\n",
      "Successfully request number 2200\n",
      "Successfully request number 2300\n",
      "Successfully request number 2400\n",
      "Successfully request number 2500\n",
      "Successfully request number 2600\n",
      "Successfully request number 2700\n",
      "Successfully request number 2800\n",
      "Successfully request number 2900\n",
      "Successfully request number 3000\n",
      "Successfully request number 3100\n",
      "Successfully request number 3200\n",
      "Successfully request number 3300\n",
      "Successfully request number 3400\n",
      "Successfully request number 3500\n",
      "Successfully request number 3600\n",
      "Successfully request number 3700\n",
      "Successfully request number 3800\n",
      "Successfully request number 3900\n",
      "Successfully request number 4000\n",
      "Successfully request number 4100\n",
      "Successfully request number 4200\n",
      "Successfully request number 4300\n",
      "Successfully request number 4400\n",
      "Successfully request number 4500\n",
      "Successfully request number 4600\n",
      "Successfully request number 4700\n",
      "Successfully request number 4800\n",
      "Successfully request number 4900\n",
      "Successfully request number 5000\n",
      "Successfully request number 5100\n",
      "Successfully request number 5200\n",
      "Successfully request number 5300\n",
      "Successfully request number 5400\n",
      "Successfully request number 5500\n",
      "Successfully request number 5600\n",
      "Successfully request number 5700\n",
      "Successfully request number 5800\n",
      "Successfully request number 5900\n",
      "Successfully request number 6000\n",
      "Successfully request number 6100\n",
      "Successfully request number 6200\n",
      "Successfully request number 6300\n",
      "Successfully request number 6400\n",
      "Successfully request number 6500\n",
      "Successfully request number 6600\n",
      "Successfully request number 6700\n",
      "Successfully request number 6800\n",
      "Successfully request number 6900\n",
      "exported to data/coins_data.csv!\n",
      "\n",
      "\n",
      "Sucessfully get data and export out to data/coins_data.csv!\n",
      "\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=90\n",
      "\n",
      "Request number: 0\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=80\n",
      "\n",
      "Request number: 1\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2710\n",
      "\n",
      "Request number: 2\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=518\n",
      "\n",
      "Request number: 3\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48543\n",
      "\n",
      "Request number: 4\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=257\n",
      "\n",
      "Request number: 5\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33285\n",
      "\n",
      "Request number: 6\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=58\n",
      "\n",
      "Request number: 7\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48537\n",
      "\n",
      "Request number: 8\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=45219\n",
      "\n",
      "Request number: 9\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=44883\n",
      "\n",
      "Request number: 10\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2\n",
      "\n",
      "Request number: 11\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=45088\n",
      "\n",
      "Request number: 12\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48591\n",
      "\n",
      "Request number: 13\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2751\n",
      "\n",
      "Request number: 14\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33422\n",
      "\n",
      "Request number: 15\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=47305\n",
      "\n",
      "Request number: 16\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=34406\n",
      "\n",
      "Request number: 17\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=1\n",
      "\n",
      "Request number: 18\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48563\n",
      "\n",
      "Request number: 19\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33830\n",
      "\n",
      "Request number: 20\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2321\n",
      "\n",
      "Request number: 21\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48571\n",
      "\n",
      "Request number: 22\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33644\n",
      "\n",
      "Request number: 23\n",
      "\n",
      "Sending request to: https://api.coinlore.net/api/coin/markets/?id=89\n",
      "\n",
      "Request number: 24\n",
      "\n",
      "exported to data/top_coins.csv!\n",
      "\n",
      "Sucessfully get data and export out to: data/top_coins.csv!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/04 20:56:02 WARN Utils: Your hostname, DESKTOP-7O0VB5H resolves to a loopback address: 127.0.1.1; using 172.21.154.35 instead (on interface eth0)\n",
      "22/01/04 20:56:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/capstone_venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d359c65b-3225-45d4-a3d5-1701c8edddd0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 226ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d359c65b-3225-45d4-a3d5-1701c8edddd0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/8ms)\n",
      "22/01/04 20:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/04 20:56:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: unix, date, symbol, open, high, low, close, Volume LTC, Volume USDT, tradecount\n",
      " Schema: unix, date, symbol, open, high, low, close, Volume BNB, Volume USDT, tradecount\n",
      "Expected: Volume BNB but found: Volume LTC\n",
      "CSV file: file:///mnt/c/Users/tienl/Udacity_Courses/coinlore_api_project/data/Downloaded_data/LTCUSDT_Binance_futures_data_minute.csv\n",
      "22/01/04 20:56:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: unix, date, symbol, open, high, low, close, Volume ETH, Volume USDT, tradecount\n",
      " Schema: unix, date, symbol, open, high, low, close, Volume BNB, Volume USDT, tradecount\n",
      "Expected: Volume BNB but found: Volume ETH\n",
      "CSV file: file:///mnt/c/Users/tienl/Udacity_Courses/coinlore_api_project/data/Downloaded_data/ETHUSDT_Binance_futures_data_minute.csv\n",
      "22/01/04 20:56:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: unix, date, symbol, open, high, low, close, Volume BTC, Volume USDT, tradecount\n",
      " Schema: unix, date, symbol, open, high, low, close, Volume BNB, Volume USDT, tradecount\n",
      "Expected: Volume BNB but found: Volume BTC\n",
      "CSV file: file:///mnt/c/Users/tienl/Udacity_Courses/coinlore_api_project/data/Downloaded_data/BTCUSDT_Binance_futures_data_minute.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sucessfully export file to: data/historical_data.csv!\n",
      "\n",
      "\n",
      "\n",
      "Sucessfully clean and export historical data to data/historical_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from requests.api import get\n",
    "from pandas_functions import *\n",
    "from  spark_functions import *\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import config as c\n",
    "import glob\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"[\n",
    "        - Data pipelines to extract, tranform and load data into S3 bucket\n",
    "        ]\n",
    "    \"\"\"\n",
    "    \n",
    "    load_dotenv()\n",
    "    # file paths\n",
    "    coin_path = 'data/coins_data.csv'\n",
    "    top_coins_path = 'data/top_coins.csv'\n",
    "    coin_exchange_path = 'data/coin_exchange_info.csv'\n",
    "    historical_data = 'data/historical_data.csv'\n",
    "    historical_data_path = \"data/Downloaded_data/*.csv\"\n",
    "    final_historical = \"data/final_historical_data.csv\"\n",
    "    # Exchange data pipeline\n",
    "    try:\n",
    "        GET_all_exchanges = 'https://api.coinlore.net/api/exchanges/'\n",
    "        exchange_requests = get_request(GET_all_exchanges)\n",
    "        exchange_data = exchange_data_filter(exchange_requests)\n",
    "        export_csv(exchange_data,coin_exchange_path)\n",
    "        print(f\"\\n\\nSucessfully get data and export out to {coin_exchange_path}!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to GET at {GET_all_exchanges}\\n')\n",
    "        print(e)\n",
    "        \n",
    "    # coins data pipeline \n",
    "    try:\n",
    "        coin_requests= get_coin_request(start=0, limit=7000)\n",
    "        export_csv(coin_requests, coin_path)\n",
    "        print(f\"\\n\\nSucessfully get data and export out to {coin_path}!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to GET at {coin_path}\\n')\n",
    "        print(e)\n",
    "\n",
    "    #top coins data pipeline\n",
    "    try:\n",
    "        top_coins_path = 'data/top_coins.csv'\n",
    "        top_coins = top_rank_coins(HowMany=25)\n",
    "        export_csv(top_coins, top_coins_path)\n",
    "        print(f\"\\nSucessfully get data and export out to: {top_coins_path}!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to GET at {top_coins_path} \\n')\n",
    "        print(e)\n",
    "\n",
    "    # binance historical coins data clean and export to csv\n",
    "    try:\n",
    "        path = r'data/historical_data.csv'\n",
    "        binance_coins_data(historical_data_path, historical_data)\n",
    "        all_files = glob.glob(path + \"/*.csv\")\n",
    "        li = []\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            li.append(df)\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv('data/final_historical_data.csv', index=False)\n",
    "        print(f\"\\nSucessfully clean and export historical data to {historical_data}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to clean and export {historical_data_path}\\n')\n",
    "        print(e) \n",
    "    \n",
    "    # Stage historical 1 minutes data to s3\n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=final_historical, s3_file_path=final_historical)\n",
    "        print(f\"\\n\\nSuccessfully Uploaded {final_historical} to S3!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {final_historical}\\n')\n",
    "        print(e)\n",
    "    \n",
    "    # stage coins_data info to s3\n",
    "    \n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=coin_path, s3_file_path=coin_path)\n",
    "        print(f\"\\nSuccessfully Uploaded {coin_path} to S3!\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {coin_path}\\n')\n",
    "        print(e)\n",
    "    \n",
    "    # stage coin market data into s3\n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=top_coins_path, s3_file_path=top_coins_path)\n",
    "        print(f\"\\n\\nSuccessfully Uploaded {top_coins_path} to S3!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {top_coins_path}\\n')\n",
    "        print(e)\n",
    "    \n",
    "    # stage coin exchange path into s3\n",
    "    try:\n",
    "        upload_to_s3(bucketname=c.S3_BUCKET_NAME, local_file_path=coin_exchange_path, s3_file_path=coin_exchange_path)\n",
    "        print(f\"\\n\\nSuccessfully Uploaded {coin_exchange_path} to S3!\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'\\nFail to upload {coin_exchange_path}\\n')\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "    with open('database/data_result_output.py', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(f\"exchange_data_s3_result={exchange_data.shape}\")\n",
    "            print(f\"coin_requests_s3_result={coin_requests.shape}\")\n",
    "            print(f\"top_coins_s3_result={top_coins.shape}\")\n",
    "            print(f\"historical_s3_result={frame.shape}\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full view of log output from main.py\n",
    "\n",
    "exported to data/coin_exchange_info.csv!\n",
    "\n",
    "\n",
    "Sucessfully get data and export out to data/coin_exchange_info.csv!\n",
    "\n",
    "\n",
    "Successfully request number 0\n",
    "Successfully request number 100\n",
    "Successfully request number 200\n",
    "Successfully request number 300\n",
    "Successfully request number 400\n",
    "Successfully request number 500\n",
    "Successfully request number 600\n",
    "Successfully request number 700\n",
    "Successfully request number 800\n",
    "Successfully request number 900\n",
    "Successfully request number 1000\n",
    "Successfully request number 1100\n",
    "Successfully request number 1200\n",
    "Successfully request number 1300\n",
    "Successfully request number 1400\n",
    "Successfully request number 1500\n",
    "Successfully request number 1600\n",
    "Successfully request number 1700\n",
    "Successfully request number 1800\n",
    "Successfully request number 1900\n",
    "Successfully request number 2000\n",
    "Successfully request number 2100\n",
    "Successfully request number 2200\n",
    "Successfully request number 2300\n",
    "Successfully request number 2400\n",
    "Successfully request number 2500\n",
    "Successfully request number 2600\n",
    "Successfully request number 2700\n",
    "Successfully request number 2800\n",
    "Successfully request number 2900\n",
    "Successfully request number 3000\n",
    "Successfully request number 3100\n",
    "Successfully request number 3200\n",
    "Successfully request number 3300\n",
    "Successfully request number 3400\n",
    "Successfully request number 3500\n",
    "Successfully request number 3600\n",
    "Successfully request number 3700\n",
    "Successfully request number 3800\n",
    "Successfully request number 3900\n",
    "Successfully request number 4000\n",
    "Successfully request number 4100\n",
    "Successfully request number 4200\n",
    "Successfully request number 4300\n",
    "Successfully request number 4400\n",
    "Successfully request number 4500\n",
    "Successfully request number 4600\n",
    "Successfully request number 4700\n",
    "Successfully request number 4800\n",
    "Successfully request number 4900\n",
    "Successfully request number 5000\n",
    "Successfully request number 5100\n",
    "Successfully request number 5200\n",
    "Successfully request number 5300\n",
    "Successfully request number 5400\n",
    "Successfully request number 5500\n",
    "Successfully request number 5600\n",
    "Successfully request number 5700\n",
    "Successfully request number 5800\n",
    "Successfully request number 5900\n",
    "Successfully request number 6000\n",
    "Successfully request number 6100\n",
    "Successfully request number 6200\n",
    "Successfully request number 6300\n",
    "Successfully request number 6400\n",
    "Successfully request number 6500\n",
    "Successfully request number 6600\n",
    "Successfully request number 6700\n",
    "Successfully request number 6800\n",
    "Successfully request number 6900\n",
    "exported to data/coins_data.csv!\n",
    "\n",
    "\n",
    "Sucessfully get data and export out to data/coins_data.csv!\n",
    "\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=90\n",
    "\n",
    "Request number: 0\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=80\n",
    "\n",
    "Request number: 1\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2710\n",
    "\n",
    "Request number: 2\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=518\n",
    "\n",
    "Request number: 3\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48543\n",
    "\n",
    "Request number: 4\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=257\n",
    "\n",
    "Request number: 5\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33285\n",
    "\n",
    "Request number: 6\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=58\n",
    "\n",
    "Request number: 7\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48537\n",
    "\n",
    "Request number: 8\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=45219\n",
    "\n",
    "Request number: 9\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=44883\n",
    "\n",
    "Request number: 10\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2\n",
    "\n",
    "Request number: 11\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=45088\n",
    "\n",
    "Request number: 12\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48591\n",
    "\n",
    "Request number: 13\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2751\n",
    "\n",
    "Request number: 14\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33422\n",
    "\n",
    "Request number: 15\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=47305\n",
    "\n",
    "Request number: 16\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=34406\n",
    "\n",
    "Request number: 17\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=1\n",
    "\n",
    "Request number: 18\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48563\n",
    "\n",
    "Request number: 19\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33830\n",
    "\n",
    "Request number: 20\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=2321\n",
    "\n",
    "Request number: 21\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=48571\n",
    "\n",
    "Request number: 22\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=33644\n",
    "\n",
    "Request number: 23\n",
    "\n",
    "Sending request to: https://api.coinlore.net/api/coin/markets/?id=89\n",
    "\n",
    "Request number: 24\n",
    "\n",
    "exported to data/top_coins.csv!\n",
    "\n",
    "Sucessfully get data and export out to: data/top_coins.csv!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **start_redshift.py** script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      ">>> Starting Redshift Cluster!\n",
      ">>> This is the configuration of redshift cluster!\n",
      "\n",
      "\n",
      "                    Param       Value\n",
      "0  DWH_CLUSTER_TYPE        multi-node\n",
      "1  DWH_NUM_NODES           4         \n",
      "2  DWH_NODE_TYPE           dc2.large \n",
      "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
      "4  DWH_DB                  dwh       \n",
      "5  DWH_DB_USER             dwhuser   \n",
      "6  DWH_DB_PASSWORD         Passw0rd  \n",
      "7  DWH_PORT                5439      \n",
      "8  DWH_IAM_ROLE_NAME       dwhRole   \n",
      "\n",
      "\n",
      "\n",
      "1.1 Creating a new IAM Role\n",
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "\n",
      "\n",
      ">>> arn:aws:iam::539761204517:role/dwhRole\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/tienl/Udacity_Courses/coinlore_api_project/pandas_functions.py:103: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "Cluster not up yet\n",
      "\n",
      "\n",
      ">>> Cluster successfully created!\n",
      "\n",
      "\n",
      ">>> DWH_ENDPOINT :: dwhcluster.cmjnfq1cdb24.us-west-2.redshift.amazonaws.com\n",
      "\n",
      "\n",
      ">>> DWH_ROLE_ARN :: arn:aws:iam::539761204517:role/dwhRole\n",
      "\n",
      "ec2.SecurityGroup(id='sg-fa39a2f3')\n",
      "\n",
      ">>> An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/capstone_venv/bin/python3.8\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pandas_functions import prettyRedshiftProps\n",
    "import config as c\n",
    "from contextlib import redirect_stdout\n",
    "import os\n",
    "\n",
    "exe_path = '/mnt/c/Users/tienl/Udacity_Courses/DE_capstone'\n",
    "os.chdir(exe_path)\n",
    "\n",
    "# (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "print(\"\\n\\n\")\n",
    "print(\">>> Starting Redshift Cluster!\\n>>> This is the configuration of redshift cluster!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [c.DWH_CLUSTER_TYPE, c.DWH_NUM_NODES, c.DWH_NODE_TYPE, c.DWH_CLUSTER_IDENTIFIER, c.DWH_DB, c.DWH_DB_USER, c.DWH_DB_PASSWORD, c.DWH_PORT, c.DWH_IAM_ROLE_NAME]\n",
    "             }))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "ec2 = c.ec2\n",
    "s3 = c.s3\n",
    "iam = c.iam\n",
    "redshift = c.redshift\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=c.DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=c.DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=c.DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=c.DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(f\"\\n\\n>>> {roleArn}\\n\\n\")\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=c.DWH_CLUSTER_TYPE,\n",
    "        NodeType=c.DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(c.DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=c.DWH_DB,\n",
    "        ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=c.DWH_DB_USER,\n",
    "        MasterUserPassword=c.DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  )\n",
    "except Exception as e:\n",
    "    print(f\"\\n >>> {e} \\n\\n\")\n",
    "    \n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "\n",
    "cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "\n",
    "check_status = cluster_df['Value'][2]\n",
    "status = 'available'\n",
    "\n",
    "\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "    cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "    check_status = cluster_df['Value'][2]\n",
    "    if check_status == status:\n",
    "        DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "        DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "        print(\"\\n\\n>>> Cluster successfully created!\\n\")\n",
    "        print(f\"\\n>>> DWH_ENDPOINT :: {DWH_ENDPOINT}\\n\")\n",
    "        print(f\"\\n>>> DWH_ROLE_ARN :: { DWH_ROLE_ARN}\\n\")\n",
    "        break\n",
    "    print('Cluster not up yet')\n",
    "\n",
    "\n",
    "with open('database/.env', 'w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        print(f'DWH_ENDPOINT={DWH_ENDPOINT}')\n",
    "        print(f'DWH_ROLE_ARN={DWH_ROLE_ARN}')\n",
    "     \n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(c.DWH_PORT),\n",
    "        ToPort=int(c.DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n>>> {e}\\n\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **db_main.py** script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n",
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS coins_data_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS exchange_data_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS top_coins_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS historical_data_table_stage!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS exchange_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      "DROP TABLE IF EXISTS coins_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      " DROP TABLE IF EXISTS historical_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      " DROP TABLE IF EXISTS top_coins_data_table!\n",
      "\n",
      "\n",
      "Sucessfully DROPPED \n",
      " DROP TABLE IF EXISTS bridge_table!\n",
      "\n",
      "\n",
      "\n",
      "Sucessfully COPY new tables!\n",
      "\n",
      "TRYING INSERT AT: \n",
      " INSERT INTO top_coins_data_table( id, name, base, quote, price, price_usd, volume, volume_usd, time)\n",
      "SELECT  id::integer\n",
      "       ,name\n",
      "       ,base\n",
      "       ,quote\n",
      "       ,price::float\n",
      "       ,price_usd::float\n",
      "       ,volume::float\n",
      "       ,volume_usd::float\n",
      "       ,time::date\n",
      "FROM top_coins_table_stage\n",
      "\n",
      "TRYING INSERT AT: \n",
      "INSERT INTO coins_data_table(id, symbol, name, rank , market_cap_usd, price_usd, price_btc)\n",
      "SELECT  id::integer\n",
      "       ,symbol\n",
      "       ,name\n",
      "       ,rank::float\n",
      "       ,replace(market_cap_usd,'0?', '0.0')::float\n",
      "       ,price_usd::float\n",
      "       ,price_btc::float\n",
      "FROM coins_data_table_stage;\n",
      "TRYING INSERT AT: \n",
      " INSERT INTO historical_data_table(unix, date, symbol, open_price, high, low, close, volume_bnb, volume_usdt, tradecount)\n",
      "SELECT  unix::varchar\n",
      "       ,date::date \n",
      "       ,split_part(symbol,'/',1) AS symbol\n",
      "       ,open_price::float\n",
      "       ,high::float\n",
      "       ,low::float\n",
      "       ,close::float\n",
      "       ,volume_bnb::float\n",
      "       ,volume_usdt::float\n",
      "       ,tradecount::integer\n",
      "FROM historical_data_table_stage\n",
      "\n",
      "TRYING INSERT AT: \n",
      "INSERT INTO exchange_data_table( id, name, url, country, date_live, volume_usd, volume_usd_adj)\n",
      "SELECT  id::numeric\n",
      "       ,name\n",
      "       ,url\n",
      "       ,country\n",
      "       ,replace(date_live,'00-00','01-01')::date AS date_live\n",
      "       ,volume_usd::numeric\n",
      "       ,volume_usd_adj::numeric\n",
      "FROM exchange_data_table_stage;\n",
      "TRYING INSERT AT: \n",
      "\n",
      "INSERT INTO bridge_table(coin_names)\n",
      "SELECT  distinct symbol\n",
      "FROM historical_data_table\n",
      "Sucessfully inserted!\n"
     ]
    }
   ],
   "source": [
    "#!/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/capstone_venv/bin/python3.8\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from create_table_sql import drop_table_queries, create_table_queries, copy_table_queries, insert_table_queries\n",
    "\n",
    "exe_path = '/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/database'\n",
    "os.chdir(exe_path)\n",
    "#Dynamic Environment variables\n",
    "load_dotenv()\n",
    "HOST = os.getenv('DWH_ENDPOINT')\n",
    "DWH_ROLE_ARB=os.getenv('DWH_ROLE_ARB')\n",
    "\n",
    "#Local and Static Environment Variables\n",
    "my_path = Path('/mnt/c/Users/tienl/Udacity_Courses/DE_capstone/.env')\n",
    "load_dotenv(my_path)\n",
    "localhost=os.getenv('localhost')\n",
    "local_dbname=os.getenv('local_dbname')\n",
    "local_password=os.getenv('local_password')\n",
    "local_username=os.getenv('local_username')\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_PORT = os.getenv(\"DWH_PORT\")\n",
    "S3_BUCKET_NAME=os.getenv('S3_BUCKET_NAME')\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function drop all tables if already exists\n",
    "    \"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        print(f\"\\nSucessfully DROPPED \\n{query}!\\n\")\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function creates table if not exists\n",
    "    \"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function load the staging tables into our DW\n",
    "    \"\"\"\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def insert_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function insert data into the tables in the DW\n",
    "    \"\"\"\n",
    "    for query in insert_table_queries:\n",
    "        print(f'TRYING INSERT AT: \\n{query}')\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Using another files with all connection string to connect into DW cluster\n",
    "    \"\"\"\n",
    "    ### Redshift database connection\n",
    "    conn = psycopg2.connect(f\"host={HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "    \n",
    "    ### Local postgres database connection\n",
    "    # conn = psycopg2.connect(host=f'{localhost}',database=f'{local_dbname}',user=f'{local_username}',password=f'{local_password}')\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        drop_tables(cur, conn)\n",
    "    except Exception as e:\n",
    "        print(f'1: \\n{e}')\n",
    "    try:      \n",
    "        create_tables(cur, conn)\n",
    "    except Exception as e:\n",
    "        print(f'2: \\n{e}')\n",
    "    try:        \n",
    "        load_staging_tables(cur,conn)\n",
    "        print(\"\\n\\nSucessfully COPY new tables!\\n\")\n",
    "    except Exception as e:\n",
    "        print(f'3: \\n{e}')\n",
    "    try:\n",
    "        insert_tables(cur,conn)\n",
    "        print(\"Sucessfully inserted!\")\n",
    "    except Exception as e:\n",
    "        print(f'4: \\n{e}')\n",
    "\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "main()\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# double click to view full log output from db_main.py\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    "DROP TABLE IF EXISTS coins_data_table_stage!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    "DROP TABLE IF EXISTS exchange_data_table_stage!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    "DROP TABLE IF EXISTS top_coins_table_stage!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    "DROP TABLE IF EXISTS historical_data_table_stage!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    "DROP TABLE IF EXISTS exchange_data_table!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    "DROP TABLE IF EXISTS coins_data_table!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    " DROP TABLE IF EXISTS historical_data_table!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    " DROP TABLE IF EXISTS top_coins_data_table!\n",
    "\n",
    "\n",
    "Sucessfully DROPPED \n",
    " DROP TABLE IF EXISTS bridge_table!\n",
    "\n",
    "\n",
    "\n",
    "Sucessfully COPY new tables!\n",
    "\n",
    "TRYING INSERT AT: \n",
    " INSERT INTO top_coins_data_table( id, name, base, quote, price, price_usd, volume, volume_usd, time)\n",
    "SELECT  id::integer\n",
    "       ,name\n",
    "       ,base\n",
    "       ,quote\n",
    "       ,price::float\n",
    "       ,price_usd::float\n",
    "       ,volume::float\n",
    "       ,volume_usd::float\n",
    "       ,time::date\n",
    "FROM top_coins_table_stage\n",
    "\n",
    "TRYING INSERT AT: \n",
    "INSERT INTO coins_data_table(id, symbol, name, rank , market_cap_usd, price_usd, price_btc)\n",
    "SELECT  id::integer\n",
    "       ,symbol\n",
    "       ,name\n",
    "       ,rank::float\n",
    "       ,replace(market_cap_usd,'0?', '0.0')::float\n",
    "       ,price_usd::float\n",
    "       ,price_btc::float\n",
    "FROM coins_data_table_stage;\n",
    "TRYING INSERT AT: \n",
    " INSERT INTO historical_data_table(unix, date, symbol, open_price, high, low, close, volume_bnb, volume_usdt, tradecount)\n",
    "SELECT  unix::varchar\n",
    "       ,date::date \n",
    "       ,split_part(symbol,'/',1) AS symbol\n",
    "       ,open_price::float\n",
    "       ,high::float\n",
    "       ,low::float\n",
    "       ,close::float\n",
    "       ,volume_bnb::float\n",
    "       ,volume_usdt::float\n",
    "       ,tradecount::integer\n",
    "FROM historical_data_table_stage\n",
    "\n",
    "TRYING INSERT AT: \n",
    "INSERT INTO exchange_data_table( id, name, url, country, date_live, volume_usd, volume_usd_adj)\n",
    "SELECT  id::numeric\n",
    "       ,name\n",
    "       ,url\n",
    "       ,country\n",
    "       ,replace(date_live,'00-00','01-01')::date AS date_live\n",
    "       ,volume_usd::numeric\n",
    "       ,volume_usd_adj::numeric\n",
    "FROM exchange_data_table_stage;\n",
    "TRYING INSERT AT: \n",
    "\n",
    "INSERT INTO bridge_table(coin_names)\n",
    "SELECT  distinct symbol\n",
    "FROM historical_data_table\n",
    "Sucessfully inserted!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **database_validation_script.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n",
      "test_1 (__main__.my_test) ... ok\n",
      "test_2 (__main__.my_test) ... ok\n",
      "test_3 (__main__.my_test) ... ok\n",
      "test_4 (__main__.my_test) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ">>> REDSHIFT DEMO'S QUERY OF TOP COINS TABLE AND EXCHANGE TABLE FOR ANALYSIS: \n",
      "                name coin_symbol         price     price_usd  \\\n",
      "0           Binance         BTC  45932.800000  45932.800000   \n",
      "1            Bitrue         BTC  45925.920000  45925.920000   \n",
      "2          Bitfinex         BTC  45905.000000  45905.000000   \n",
      "3            Indoex         BTC  46040.521403  46040.521403   \n",
      "4             TOKOK         BTC  45913.080000  45913.080000   \n",
      "..              ...         ...           ...           ...   \n",
      "832    VCC Exchange         FTM      1.689500      1.689500   \n",
      "833          OMGFIN         FTM      0.000038      1.732410   \n",
      "834            Bkex         XLM      0.328600      0.328600   \n",
      "835    Coinbase Pro         XLM      0.278989      0.329890   \n",
      "836  Bithumb Global         XLM      0.328109      0.328109   \n",
      "\n",
      "                           url                                 country  \\\n",
      "0      https://www.binance.com                                   Japan   \n",
      "1      https://www.bitrue.com/                               Singapore   \n",
      "2     https://www.bitfinex.com                               Hong Kong   \n",
      "3            https://indoex.io                                 Estonia   \n",
      "4       https://www.tokok.com/                  British Virgin Islands   \n",
      "..                         ...                                     ...   \n",
      "832       https://vcc.exchange                                 Vietnam   \n",
      "833         https://omgfin.com                 Estonia European Union    \n",
      "834      https://www.bkex.com/                  British Virgin Islands   \n",
      "835  https://www.coinbase.com/  San Francisco California United States   \n",
      "836   https://www.bithumb.pro/                              Seychelles   \n",
      "\n",
      "      date_live        volume  \n",
      "0    2014-01-01  6.494519e+04  \n",
      "1    2018-01-01  1.178438e+04  \n",
      "2    2012-01-01  7.568765e+03  \n",
      "3    2018-01-01  4.446091e+03  \n",
      "4    2018-01-01  3.452379e+03  \n",
      "..          ...           ...  \n",
      "832  2019-01-01  4.259107e+06  \n",
      "833  2018-10-10  3.478680e+05  \n",
      "834  2018-01-01  6.358333e+07  \n",
      "835  2012-06-20  2.956410e+07  \n",
      "836  2019-01-01  1.661179e+07  \n",
      "\n",
      "[837 rows x 8 columns] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> REDSHIFT DATABASE STAGING TABLE'S SHAPE RESULT:  (7000, 15) (290, 11) (1011, 9) (2713925, 10) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import psycopg2\n",
    "from pyspark.sql.functions import row_number\n",
    "from data_result_output import *\n",
    "import pandas as pd\n",
    "from db_main import DB_NAME, DB_USER, DB_PASSWORD, DB_PORT, S3_BUCKET_NAME, HOST, DWH_ROLE_ARB, localhost, local_dbname, local_username, local_password\n",
    "from create_table_sql import validation_queries\n",
    "\n",
    "\n",
    "col_names = []\n",
    "my_list = []\n",
    "# conn = psycopg2.connect(host=f'{localhost}',database=f'{local_dbname}',user=f'{local_username}',password=f'{local_password}')\n",
    "\n",
    "conn = psycopg2.connect(f\"host={HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "cur = conn.cursor()\n",
    "for query in validation_queries:\n",
    "    cur.execute(query)\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "    row_num = cur.fetchall()\n",
    "    col_names.append(colnames)\n",
    "    my_list.append(row_num)\n",
    "    conn.commit()\n",
    "conn.close()\n",
    "\n",
    "coin_row, exchange_row, top_coin_row, historical_row, redshift_demo_query_row = my_list\n",
    "coin_col, exchange_col, top_coin_col, historical_col, redshift_demo_query_col = col_names\n",
    "\n",
    "coin_shape = (len(coin_row), len(coin_col))\n",
    "exchange_shape = (len(exchange_row), len(exchange_col))\n",
    "top_coin_shape = (len(top_coin_row), len(top_coin_col))\n",
    "historical_shape = (len(historical_row), len(historical_col))\n",
    "\n",
    "redshift_demo_query = pd.DataFrame(redshift_demo_query_row, columns=redshift_demo_query_col)\n",
    "print(\"\\n\\n>>> Redshift Demo's query of top coins table and exchange table for analysis: \\n\".upper(), redshift_demo_query, \"\\n\\n\")\n",
    "print(\"\\n\\n>>> Redshift DataBase staging table's shape result: \".upper(),coin_shape, exchange_shape, top_coin_shape, historical_shape, \"\\n\\n\")\n",
    "\n",
    "class my_test(unittest.TestCase):    \n",
    "    \n",
    "    def test_1(self):\n",
    "        self.assertEqual(coin_requests_s3_result, coin_shape, f\"\\n\\n>>> Test coin_data_result_s3 result should be:{coin_requests_s3_result}\")\n",
    "    \n",
    "    def test_2(self):\n",
    "        self.assertEqual(exchange_data_s3_result, exchange_shape, f\"\\n\\n>>> Test exchange_data_result_s3 result should be:{exchange_data_s3_result}\")\n",
    "\n",
    "    def test_3(self):\n",
    "        self.assertEqual(historical_s3_result, historical_shape,f\"\\n\\n>>> Test historical_data_result_s3 result should be:{historical_s3_result}\")\n",
    "    \n",
    "    def test_4(self):\n",
    "        self.assertEqual(top_coins_s3_result, top_coin_shape, f\"\\n\\n>>> Test top_coins_data_result_s3 result should be: {top_coins_s3_result}\")\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(my_test)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dobule click to view full Logs output from database_validation_script.py\n",
    "\n",
    "\n",
    ">>> REDSHIFT DEMO'S QUERY OF TOP COINS TABLE AND EXCHANGE TABLE FOR ANALYSIS: \n",
    "                name coin_symbol         price     price_usd  \\\n",
    "0           Binance         BTC  45932.800000  45932.800000   \n",
    "1            Bitrue         BTC  45925.920000  45925.920000   \n",
    "2          Bitfinex         BTC  45905.000000  45905.000000   \n",
    "3            Indoex         BTC  46040.521403  46040.521403   \n",
    "4             TOKOK         BTC  45913.080000  45913.080000   \n",
    "..              ...         ...           ...           ...   \n",
    "832    VCC Exchange         FTM      1.689500      1.689500   \n",
    "833          OMGFIN         FTM      0.000038      1.732410   \n",
    "834            Bkex         XLM      0.328600      0.328600   \n",
    "835    Coinbase Pro         XLM      0.278989      0.329890   \n",
    "836  Bithumb Global         XLM      0.328109      0.328109   \n",
    "\n",
    "                           url                                 country  \\\n",
    "0      https://www.binance.com                                   Japan   \n",
    "1      https://www.bitrue.com/                               Singapore   \n",
    "2     https://www.bitfinex.com                               Hong Kong   \n",
    "3            https://indoex.io                                 Estonia   \n",
    "4       https://www.tokok.com/                  British Virgin Islands   \n",
    "..                         ...                                     ...   \n",
    "832       https://vcc.exchange                                 Vietnam   \n",
    "833         https://omgfin.com                 Estonia European Union    \n",
    "834      https://www.bkex.com/                  British Virgin Islands   \n",
    "835  https://www.coinbase.com/  San Francisco California United States   \n",
    "836   https://www.bithumb.pro/                              Seychelles   \n",
    "\n",
    "      date_live        volume  \n",
    "0    2014-01-01  6.494519e+04  \n",
    "1    2018-01-01  1.178438e+04  \n",
    "2    2012-01-01  7.568765e+03  \n",
    "3    2018-01-01  4.446091e+03  \n",
    "4    2018-01-01  3.452379e+03  \n",
    "..          ...           ...  \n",
    "832  2019-01-01  4.259107e+06  \n",
    "833  2018-10-10  3.478680e+05  \n",
    "834  2018-01-01  6.358333e+07  \n",
    "835  2012-06-20  2.956410e+07  \n",
    "836  2019-01-01  1.661179e+07  \n",
    "\n",
    "[837 rows x 8 columns] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">>> REDSHIFT DATABASE STAGING TABLE'S SHAPE RESULT:  (7000, 15) (290, 11) (1011, 9) (2713925, 10) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **stop_redshift.py** script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 30\n",
      "/mnt/c/Users/tienl/Udacity_Courses/coinlore_api_project/pandas_functions.py:103: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> Cluster status: deleting!\n",
      "\n",
      "\n",
      "\n",
      ">>> No cluster found!\n",
      ">>> Cluster deleted!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas_functions import prettyRedshiftProps\n",
    "import config as c\n",
    "\n",
    "def main():\n",
    "    \n",
    "    status = 'deleting'\n",
    "    c.redshift.delete_cluster( ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "    c.iam.detach_role_policy(RoleName=c.DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    c.iam.delete_role(RoleName=c.DWH_IAM_ROLE_NAME)\n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        myClusterProps = c.redshift.describe_clusters(ClusterIdentifier=c.DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "        cluster_df = prettyRedshiftProps(myClusterProps)\n",
    "        check_status = cluster_df['Value'][2]\n",
    "        # check_status = cluster_df['Value'][2]\n",
    "        try:\n",
    "            if check_status == status:\n",
    "                print(f'\\n\\n>>> Cluster status: {check_status}!\\n')\n",
    "        except Exception as e:\n",
    "            if e == e:\n",
    "                print('\\n\\n>>> Cluster deleted!\\n')\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        if e ==e:\n",
    "            print('\\n\\n>>> No cluster found!\\n>>> Cluster deleted!\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ffc6fd8cb3e14e16b5dc077588a22483e644787fd42fd05cc460417ce2d8385"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('capstone_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
